{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bwilab/.local/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/bwilab/.local/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "sys.path.append(\"./Grounded_Segment_Anything/recognize-anything\")\n",
    "sys.path.append(\"./Grounded_Segment_Anything/GroundingDINO\")\n",
    "sys.path.append(\"./Grounded_Segment_Anything/segment_anything\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import pykinect_azure as pykinect\n",
    "\n",
    "# Grounding DINO\n",
    "import Grounded_Segment_Anything.GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.models import build_model\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.utils import (\n",
    "    clean_state_dict, \n",
    "    get_phrases_from_posmap\n",
    ")\n",
    "\n",
    "# Segment Anything\n",
    "from Grounded_Segment_Anything.segment_anything.segment_anything import (\n",
    "    sam_model_registry,\n",
    "    SamPredictor\n",
    ")\n",
    "\n",
    "# Recognize Anything Model & Tag2Text\n",
    "from ram.models import ram\n",
    "from ram import inference_ram\n",
    "import torchvision.transforms as TS\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from projections import PointProjector\n",
    "from aggregator import PointCloudAggregator\n",
    "\n",
    "GROUNDING_DINO_CONFIG = \"Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"Grounded_Segment_Anything/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"Grounded_Segment_Anything/sam_vit_h_4b8939.pth\"\n",
    "RAM_CHECKPOINT = \"Grounded_Segment_Anything/ram_swin_large_14m.pth\"\n",
    "BOX_THRESHOLD = 0.50\n",
    "TEXT_THRESHOLD = 0.50\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BERT_BASE_UNCASED_PATH = None\n",
    "\n",
    "# device_config = pykinect.default_configuration\n",
    "# device_config.color_format = pykinect.K4A_IMAGE_FORMAT_COLOR_BGRA32\n",
    "# device_config.color_resolution = pykinect.K4A_COLOR_RESOLUTION_720P\n",
    "# device_config.depth_mode = pykinect.K4A_DEPTH_MODE_NFOV_2X2BINNED\n",
    "\n",
    "# pykinect.initialize_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/encoder/layer/0/crossattention/self/query is tied\n",
      "/encoder/layer/0/crossattention/self/key is tied\n",
      "/encoder/layer/0/crossattention/self/value is tied\n",
      "/encoder/layer/0/crossattention/output/dense is tied\n",
      "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/0/intermediate/dense is tied\n",
      "/encoder/layer/0/output/dense is tied\n",
      "/encoder/layer/0/output/LayerNorm is tied\n",
      "/encoder/layer/1/crossattention/self/query is tied\n",
      "/encoder/layer/1/crossattention/self/key is tied\n",
      "/encoder/layer/1/crossattention/self/value is tied\n",
      "/encoder/layer/1/crossattention/output/dense is tied\n",
      "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/1/intermediate/dense is tied\n",
      "/encoder/layer/1/output/dense is tied\n",
      "/encoder/layer/1/output/LayerNorm is tied\n",
      "--------------\n",
      "Grounded_Segment_Anything/ram_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from Grounded_Segment_Anything/ram_swin_large_14m.pth\n",
      "vit: swin_l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RAM(\n",
       "  (visual_encoder): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        dim=192, input_resolution=(96, 96), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(96, 96), num_heads=6, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(12, 12), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(96, 96), num_heads=6, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(12, 12), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(96, 96), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=384, input_resolution=(48, 48), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(48, 48), num_heads=12, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(12, 12), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(48, 48), num_heads=12, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(12, 12), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(48, 48), dim=384\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=768, input_resolution=(24, 24), depth=18\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(12, 12), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(24, 24), dim=768\n",
       "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=1536, input_resolution=(12, 12), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            dim=1536, input_resolution=(12, 12), num_heads=48, window_size=12, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=1536, window_size=(12, 12), num_heads=48\n",
       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (tag_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tagging_head): BertModel(\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (wordvec_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (tagging_loss_function): AsymmetricLoss()\n",
       "  (image_proj): Linear(in_features=1536, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rotation_translation(filepath) -> dict[np.ndarray]: ## filename -> transform\n",
    "    world_transforms = dict()\n",
    "    \n",
    "    skip_header = True\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            if skip_header:\n",
    "                skip_header = False\n",
    "                continue\n",
    "\n",
    "            data = line.split()[1:] # ignore timestamp\n",
    "            filename = data.pop() + \".png\"\n",
    "            data = [float(p) for p in data]\n",
    "            rigid_transform = quaternion_to_rigid_transform(*data)\n",
    "            \n",
    "            ## first do E, then try E inverse\n",
    "            E = np.eye(4)\n",
    "            E[:3] = rigid_transform\n",
    "            world_transforms[filename] = E\n",
    "\n",
    "    return world_transforms\n",
    "\n",
    "def quaternion_to_rigid_transform(x, y, z, qx, qy, qz, qw) -> np.ndarray:\n",
    "    # Normalize the quaternion\n",
    "    norm = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "    qx, qy, qz, qw = qx / norm, qy / norm, qz / norm, qw / norm\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    E_inv = np.zeros((3, 4))\n",
    "    R = np.array([\n",
    "        [1 - 2*(qy**2 + qz**2), 2*(qx*qy - qz*qw),     2*(qx*qz + qy*qw)],\n",
    "        [2*(qx*qy + qz*qw),     1 - 2*(qx**2 + qz**2), 2*(qy*qz - qx*qw)],\n",
    "        [2*(qx*qz - qy*qw),     2*(qy*qz + qx*qw),     1 - 2*(qx**2 + qy**2)]\n",
    "    ])\n",
    "\n",
    "    E_inv[:3, :3] = R\n",
    "    E_inv[:, 3] = np.array([x, y, z])\n",
    "    return E_inv\n",
    "\n",
    "def prepare_image(image: np.ndarray):\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image_pil = Image.fromarray(image)\n",
    "    image_tensor, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image_tensor\n",
    "\n",
    "def load_grounding_dino(model_config_path, model_checkpoint_path, bert_base_uncased_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    args.bert_base_uncased_path = bert_base_uncased_path\n",
    "\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold, with_logits=True, device=\"cpu\"):\n",
    "    caption = caption.lower().strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption += \".\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "        logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]\n",
    "        boxes = outputs[\"pred_boxes\"].cpu()[0]\n",
    "\n",
    "    filt_mask = logits.max(dim=1)[0] > box_threshold\n",
    "    logits_filt = logits[filt_mask]\n",
    "    boxes_filt = boxes[filt_mask]\n",
    "\n",
    "    tokenized = model.tokenizer(caption)\n",
    "    pred_phrases = [\n",
    "        get_phrases_from_posmap(logit > text_threshold, tokenized, model.tokenizer) +\n",
    "        (f\"({str(logit.max().item())[:4]})\" if with_logits else \"\")\n",
    "        for logit, _ in zip(logits_filt, boxes_filt)\n",
    "    ]\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "def get_ram_output(model, image_pil):\n",
    "    normalize = TS.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "    transform = TS.Compose([\n",
    "                    TS.Resize((384, 384)),\n",
    "                    TS.ToTensor(), normalize\n",
    "                ])    \n",
    "    \n",
    "    raw_image = image_pil.resize((384, 384))\n",
    "    raw_image  = transform(raw_image).unsqueeze(0).to(DEVICE)\n",
    "    res = inference_ram(raw_image, model)\n",
    "    tags = res[0].replace(' |', '.')\n",
    "    return tags\n",
    "\n",
    "gd_model = load_grounding_dino(GROUNDING_DINO_CONFIG, GROUNDING_DINO_CHECKPOINT, BERT_BASE_UNCASED_PATH, device=DEVICE)\n",
    "sam_model = SamPredictor(sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE))\n",
    "ram_model = ram(pretrained=RAM_CHECKPOINT,\n",
    "                image_size=384,\n",
    "                vit='swin_l',\n",
    "                threshold=0.68).to(DEVICE)\n",
    "ram_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 1/52\n",
      "Processing frame 2/52\n",
      "Processing frame 3/52\n",
      "Processing frame 4/52\n",
      "Processing frame 5/52\n",
      "Processing frame 6/52\n",
      "Processing frame 7/52\n",
      "Processing frame 8/52\n",
      "Processing frame 9/52\n",
      "Processing frame 10/52\n",
      "Processing frame 11/52\n",
      "Processing frame 12/52\n",
      "Processing frame 13/52\n",
      "Processing frame 14/52\n",
      "Processing frame 15/52\n",
      "Processing frame 16/52\n",
      "Processing frame 17/52\n",
      "Processing frame 18/52\n",
      "Processing frame 19/52\n",
      "Processing frame 20/52\n",
      "Processing frame 21/52\n",
      "Processing frame 22/52\n",
      "Processing frame 23/52\n",
      "Processing frame 24/52\n",
      "Processing frame 25/52\n",
      "Processing frame 26/52\n",
      "Processing frame 27/52\n",
      "Processing frame 28/52\n",
      "Processing frame 29/52\n",
      "Processing frame 30/52\n",
      "Processing frame 31/52\n",
      "Processing frame 32/52\n",
      "Processing frame 33/52\n",
      "Processing frame 34/52\n",
      "Processing frame 35/52\n",
      "Processing frame 36/52\n",
      "Processing frame 37/52\n",
      "Processing frame 38/52\n",
      "Processing frame 39/52\n",
      "Processing frame 40/52\n",
      "Processing frame 41/52\n",
      "Processing frame 42/52\n",
      "Processing frame 43/52\n",
      "Processing frame 44/52\n",
      "Processing frame 45/52\n",
      "Processing frame 46/52\n",
      "Processing frame 47/52\n",
      "Processing frame 48/52\n",
      "Processing frame 49/52\n",
      "Processing frame 50/52\n",
      "Processing frame 51/52\n",
      "Processing frame 52/52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['microwave(0.78)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## must have /rgb, /depth directories and /poses.txt file\n",
    "DIRECTORY = \"/home/bwilab/Documents/RTAB-Map/kitchen\"\n",
    "# DIRECTORY = \"/home/bwilab/Semantic-Mapping-BWI/red_chair\"\n",
    "object_tags = \"box. cardboard box. electronic. equipment. office supply. package.\"\n",
    "\n",
    "pointclouds = defaultdict(list) # Object label --> list[PointCloud]\n",
    "projector = PointProjector()\n",
    "aggregator = PointCloudAggregator(eps=0.50)\n",
    "\n",
    "all_pointclouds = []\n",
    "\n",
    "world_transforms = get_rotation_translation(f\"{DIRECTORY}/poses.txt\")\n",
    "file_basenames = [os.path.basename(file) for file in os.listdir(f\"{DIRECTORY}/rgb\")]\n",
    "file_basenames.sort(key=lambda x: int(x[:-4]))\n",
    "rgb_images = [f\"{DIRECTORY}/rgb/{file}\" for file in file_basenames]\n",
    "depth_images = [f\"{DIRECTORY}/depth/{file}\" for file in file_basenames]\n",
    "assert(len(rgb_images) == len(depth_images) == len(world_transforms))\n",
    "\n",
    "# rgb_images = rgb_images[:1]\n",
    "# depth_images = depth_images[:1]\n",
    "\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(16, 8))\n",
    "# axs[0, 0].set_axis_off(); axs[0, 1].set_axis_off(); axs[1, 0].set_axis_off(); axs[1, 1].set_axis_off()\n",
    "# def show_box(box, ax, label):\n",
    "#     x0, y0 = box[0], box[1]\n",
    "#     w, h = box[2] - box[0], box[3] - box[1]\n",
    "#     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "#     ax.text(x0, y0, label)\n",
    "\n",
    "for frame_idx, (rgb_path, depth_path) in enumerate(zip(rgb_images, depth_images)):\n",
    "    print(f\"Processing frame {frame_idx+1}/{len(rgb_images)}\")\n",
    "    scene = defaultdict(list) # Object label --> list[masks]\n",
    "    transform = world_transforms[os.path.basename(rgb_path)]\n",
    "\n",
    "    ## Make sure frames match\n",
    "    assert os.path.basename(rgb_path) == os.path.basename(depth_path)\n",
    "\n",
    "    with Image.open(rgb_path) as color_image, Image.open(depth_path) as depth_image:\n",
    "        ## Feed through RAM\n",
    "        object_tags = get_ram_output(ram_model, color_image)\n",
    "        object_tags += \" floor.\"\n",
    "\n",
    "        ## Make sure images are same dims\n",
    "        color_image, depth_image = np.array(color_image), np.array(depth_image)\n",
    "        resized_color_image = cv2.resize(color_image, depth_image.shape[::-1])\n",
    "    \n",
    "    ## Feed through DINO\n",
    "    image_pil, image_tensor = prepare_image(resized_color_image)\n",
    "    boxes, pred_phrases = get_grounding_output(\n",
    "        gd_model, image_tensor, object_tags, BOX_THRESHOLD, TEXT_THRESHOLD, device=DEVICE\n",
    "    )\n",
    "\n",
    "    ## Prepare SAM\n",
    "    if torch.numel(boxes) == 0: # nothing found in frame\n",
    "        continue\n",
    "    \n",
    "    sam_model.set_image(resized_color_image)\n",
    "    W, H = image_pil.size\n",
    "    for i in range(boxes.size(0)):\n",
    "        boxes[i] *= torch.Tensor([W, H, W, H])\n",
    "        boxes[i][:2] -= boxes[i][2:] / 2\n",
    "        boxes[i][2:] += boxes[i][:2]\n",
    "\n",
    "    ## SAM outputs\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes, resized_color_image.shape[:2]).to(DEVICE)\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None, point_labels=None, boxes=transformed_boxes.to(DEVICE), multimask_output=False\n",
    "    )\n",
    "\n",
    "    ## Associate outputs\n",
    "        ## How to use\n",
    "        # color_pixels = color_image * mask[:, :, None]\n",
    "        # depth_pixels = depth_image * mask\n",
    "    for mask, box, label in zip(masks, boxes, pred_phrases):\n",
    "        mask = mask[0].cpu().numpy()\n",
    "        x0, y0 = box[0], box[1]\n",
    "        label = label[:label.index('(')] # remove confidence\n",
    "        scene[label].append(mask)\n",
    "\n",
    "    all_masks = np.zeros_like(resized_color_image)\n",
    "    ## Generating pointclouds\n",
    "    for label in scene:\n",
    "        for mask in scene[label]:\n",
    "            masked_depth_image = depth_image * mask\n",
    "            masked_color_image = resized_color_image * mask[:, :, None]\n",
    "            all_masks |= mask[:, :, None]\n",
    "            # plt.imshow(masked_color_image)\n",
    "            # plt.show()\n",
    "            # plt.imshow(masked_depth_image)\n",
    "            # plt.show()\n",
    "\n",
    "            pcl = projector.get_pointcloud(masked_depth_image, masked_color_image, stride=3)\n",
    "            pcl.clean(verbose=False)\n",
    "            if pcl.is_empty():\n",
    "                continue\n",
    "\n",
    "            pcl.label = label\n",
    "            pcl.transformation = transform\n",
    "            pcl.transform(transform)\n",
    "            instance = aggregator.nearest_pointcloud(pcl)\n",
    "            aggregator.aggregate_pointcloud(pcl, instance)\n",
    "\n",
    "    # axs[0, 0].imshow(resized_color_image) # camera input\n",
    "    # axs[0, 1].imshow(depth_image)\n",
    "    # axs[1, 0].imshow(resized_color_image)\n",
    "    # for box, label in zip(boxes, pred_phrases):\n",
    "    #     show_box(box, axs[1, 0], label)\n",
    "    # axs[1, 1].imshow(resized_color_image * all_masks) # sam frame\n",
    "    # plt.show()\n",
    "\n",
    "# aggregator.refine_views()\n",
    "# projector.visualize(aggregator.main)\n",
    "\n",
    "# aggregator.main.transform_to_rtab().save(\"semantic_blue_chair.ply\")\n",
    "\n",
    "pred_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# agg = deepcopy(aggregator)\n",
    "# agg.refine_views()\n",
    "# projector.visualize(agg.scene['package'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pointcloud import PointCloud\n",
    "import random\n",
    "agg = deepcopy(aggregator)\n",
    "\n",
    "for key in agg.scene:\n",
    "    a = PointCloud()\n",
    "    r,g,b = random.random(), random.random(), random.random()\n",
    "    for pcl in agg.scene[key]:\n",
    "        random_color = np.array([[r,g,b] for i in range(len(pcl))])\n",
    "        pcl.colors = random_color\n",
    "        a += pcl\n",
    "    a.save(f\"kitchen/{key}.ply\")\n",
    "\n",
    "projector.visualize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg = deepcopy(aggregator)\n",
    "\n",
    "# for instance in agg._scene['package']:\n",
    "#     for view in instance:\n",
    "#         random_color = np.random.uniform(size=(3,))\n",
    "#         for pcl in view.pointclouds:\n",
    "#             pcl._pcl.paint_uniform_color(random_color)\n",
    "\n",
    "# all_views = []\n",
    "# for instance in agg._scene['package']:\n",
    "#     for view in instance:\n",
    "#         all_views += [view.get_pointcloud()]\n",
    "            \n",
    "\n",
    "# projector.visualize(all_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pointcloud import PointCloud\n",
    "# agg = deepcopy(aggregator)\n",
    "\n",
    "# all_pointclouds = []\n",
    "# for label in agg._scene:\n",
    "#     for instance in agg._scene[label]:\n",
    "#         new_instance = PointCloud()\n",
    "#         random_color = np.random.uniform(size=(3,))\n",
    "\n",
    "#         for view in instance:\n",
    "#             new_instance += view.get_pointcloud()\n",
    "\n",
    "#         new_instance._pcl.paint_uniform_color(random_color)\n",
    "#         all_pointclouds += [new_instance]\n",
    "\n",
    "# projector.visualize(all_pointclouds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
