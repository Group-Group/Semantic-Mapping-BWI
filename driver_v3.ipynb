{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"./Grounded_Segment_Anything/recognize-anything\")\n",
    "sys.path.append(\"./Grounded_Segment_Anything/GroundingDINO\")\n",
    "sys.path.append(\"./Grounded_Segment_Anything/segment_anything\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import pykinect_azure as pykinect\n",
    "import open3d as o3d\n",
    "\n",
    "# Grounding DINO\n",
    "import Grounded_Segment_Anything.GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.models import build_model\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.utils import (\n",
    "    clean_state_dict, \n",
    "    get_phrases_from_posmap\n",
    ")\n",
    "\n",
    "# Segment Anything\n",
    "from Grounded_Segment_Anything.segment_anything.segment_anything import (\n",
    "    sam_model_registry,\n",
    "    SamPredictor\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from pointcloud import PointCloud\n",
    "from projections import PointProjector\n",
    "from aggregator import PointCloudAggregator\n",
    "\n",
    "GROUNDING_DINO_CONFIG = \"Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"Grounded_Segment_Anything/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"Grounded_Segment_Anything/sam_vit_h_4b8939.pth\"\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BERT_BASE_UNCASED_PATH = None\n",
    "\n",
    "# device_config = pykinect.default_configuration\n",
    "# device_config.color_format = pykinect.K4A_IMAGE_FORMAT_COLOR_BGRA32\n",
    "# device_config.color_resolution = pykinect.K4A_COLOR_RESOLUTION_720P\n",
    "# device_config.depth_mode = pykinect.K4A_DEPTH_MODE_NFOV_2X2BINNED\n",
    "\n",
    "# pykinect.initialize_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_translation(filepath) -> dict[np.ndarray]: ## filename -> transform\n",
    "    world_transforms = dict()\n",
    "    \n",
    "    skip_header = True\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            if skip_header:\n",
    "                skip_header = False\n",
    "                continue\n",
    "\n",
    "            data = line.split()[1:] # ignore timestamp\n",
    "            filename = data.pop() + \".png\"\n",
    "            data = [float(p) for p in data]\n",
    "            rigid_transform = quaternion_to_rigid_transform(*data)\n",
    "            \n",
    "            ## first do E, then try E inverse\n",
    "            E = np.eye(4)\n",
    "            E[:3] = rigid_transform\n",
    "            world_transforms[filename] = E\n",
    "\n",
    "    return world_transforms\n",
    "\n",
    "def quaternion_to_rigid_transform(x, y, z, qx, qy, qz, qw) -> np.ndarray:\n",
    "    # Normalize the quaternion\n",
    "    norm = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "    qx, qy, qz, qw = qx / norm, qy / norm, qz / norm, qw / norm\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    E_inv = np.zeros((3, 4))\n",
    "    R = np.array([\n",
    "        [1 - 2*(qy**2 + qz**2), 2*(qx*qy - qz*qw),     2*(qx*qz + qy*qw)],\n",
    "        [2*(qx*qy + qz*qw),     1 - 2*(qx**2 + qz**2), 2*(qy*qz - qx*qw)],\n",
    "        [2*(qx*qz - qy*qw),     2*(qy*qz + qx*qw),     1 - 2*(qx**2 + qy**2)]\n",
    "    ])\n",
    "\n",
    "    E_inv[:3, :3] = R\n",
    "    E_inv[:, 3] = np.array([x, y, z])\n",
    "    return E_inv\n",
    "\n",
    "def prepare_image(image: np.ndarray):\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image_pil = Image.fromarray(image)\n",
    "    image_tensor, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image_tensor\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, bert_base_uncased_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    args.bert_base_uncased_path = bert_base_uncased_path\n",
    "\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold, with_logits=True, device=\"cpu\"):\n",
    "    caption = caption.lower().strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption += \".\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "        logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]\n",
    "        boxes = outputs[\"pred_boxes\"].cpu()[0]\n",
    "\n",
    "    filt_mask = logits.max(dim=1)[0] > box_threshold\n",
    "    logits_filt = logits[filt_mask]\n",
    "    boxes_filt = boxes[filt_mask]\n",
    "\n",
    "    tokenized = model.tokenizer(caption)\n",
    "    pred_phrases = [\n",
    "        get_phrases_from_posmap(logit > text_threshold, tokenized, model.tokenizer) +\n",
    "        (f\"({str(logit.max().item())[:4]})\" if with_logits else \"\")\n",
    "        for logit, _ in zip(logits_filt, boxes_filt)\n",
    "    ]\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "gd_model = load_model(GROUNDING_DINO_CONFIG, GROUNDING_DINO_CHECKPOINT, BERT_BASE_UNCASED_PATH, device=DEVICE)\n",
    "sam_model = SamPredictor(sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## must have /rgb, /depth directories and /poses.txt file\n",
    "DIRECTORY = \"/home/bwilab/Semantic-Mapping-BWI/red_chair\"\n",
    "OBJECT_LABELS = \"couch.\"\n",
    "\n",
    "pointclouds = defaultdict(list) # Object label --> list[PointCloud]\n",
    "projector = PointProjector()\n",
    "aggregator = PointCloudAggregator(eps=0.05) ## higher eps == more merging, lower eps == more detail (or noise)\n",
    "\n",
    "all_pointclouds = []\n",
    "\n",
    "world_transforms = get_rotation_translation(f\"{DIRECTORY}/poses.txt\")\n",
    "file_basenames = [os.path.basename(file) for file in os.listdir(f\"{DIRECTORY}/rgb\")]\n",
    "file_basenames.sort(key=lambda x: int(x[:-4]))\n",
    "rgb_images = [f\"{DIRECTORY}/rgb/{file}\" for file in file_basenames]\n",
    "depth_images = [f\"{DIRECTORY}/depth/{file}\" for file in file_basenames]\n",
    "assert(len(rgb_images) == len(depth_images) == len(world_transforms))\n",
    "\n",
    "rgb_images = rgb_images[:15]\n",
    "depth_images = depth_images[:15]\n",
    "\n",
    "for i, (rgb_path, depth_path) in enumerate(zip(rgb_images, depth_images)):\n",
    "    print(f\"Processing frame {i+1}/{len(rgb_images)}\")\n",
    "    scene = defaultdict(list) # Object label --> list[masks]\n",
    "    transform = world_transforms[os.path.basename(rgb_path)]\n",
    "\n",
    "    ## Make sure frames match\n",
    "    assert os.path.basename(rgb_path) == os.path.basename(depth_path)\n",
    "\n",
    "    with Image.open(rgb_path) as color_image, Image.open(depth_path) as depth_image:\n",
    "        ## Make sure images are same dims\n",
    "        color_image, depth_image = np.array(color_image), np.array(depth_image)\n",
    "        resized_color_image = cv2.resize(color_image, depth_image.shape[::-1])\n",
    "\n",
    "    ## Feed through DINO\n",
    "    image_pil, image_tensor = prepare_image(resized_color_image)\n",
    "    boxes, pred_phrases = get_grounding_output(\n",
    "        gd_model, image_tensor, OBJECT_LABELS, BOX_THRESHOLD, TEXT_THRESHOLD, device=DEVICE\n",
    "    )\n",
    "\n",
    "    ## Prepare SAM\n",
    "    if torch.numel(boxes) == 0: # nothing found in frame\n",
    "        continue\n",
    "    \n",
    "    sam_model.set_image(resized_color_image)\n",
    "    W, H = image_pil.size\n",
    "    for i in range(boxes.size(0)):\n",
    "        boxes[i] *= torch.Tensor([W, H, W, H])\n",
    "        boxes[i][:2] -= boxes[i][2:] / 2\n",
    "        boxes[i][2:] += boxes[i][:2]\n",
    "\n",
    "    ## SAM outputs\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes, resized_color_image.shape[:2]).to(DEVICE)\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None, point_labels=None, boxes=transformed_boxes.to(DEVICE), multimask_output=False\n",
    "    )\n",
    "\n",
    "    ## Associate outputs\n",
    "        ## How to use\n",
    "        # color_pixels = color_image * mask[:, :, None]\n",
    "        # depth_pixels = depth_image * mask\n",
    "    for mask, box, label in zip(masks, boxes, pred_phrases):\n",
    "        mask = mask[0].cpu().numpy()\n",
    "        x0, y0 = box[0], box[1]\n",
    "        label = label[:label.index('(')] # remove confidence\n",
    "        scene[label].append(mask)\n",
    "\n",
    "    ## Generating pointclouds\n",
    "    for label in scene:\n",
    "        for mask in scene[label]:\n",
    "            masked_depth_image = depth_image * mask\n",
    "            masked_color_image = resized_color_image * mask[:, :, None]\n",
    "            # plt.imshow(masked_color_image)\n",
    "            # plt.show()\n",
    "            # plt.imshow(masked_depth_image)\n",
    "            # plt.show()\n",
    "\n",
    "            pcl = projector.get_pointcloud(masked_depth_image, masked_color_image, stride=3)\n",
    "            pcl.label = label\n",
    "            # pcl.clean() # todo this method removes color\n",
    "            if pcl.is_empty():\n",
    "                continue\n",
    "\n",
    "            pcl.transform(transform)\n",
    "            target = aggregator.nearest_pointcloud(pcl)\n",
    "            aggregator.aggregate_pointcloud(pcl, target)\n",
    "            \n",
    "            # aggregator._unmerged_pointclouds[label] += [pcl]\n",
    "            # {'chair': [[pcl1, pcl2], [pcl3]]}\n",
    "            # {'chair': [pcl1_done, pcl2_done]}\n",
    "            \n",
    "            ## TESTING 11/22\n",
    "            # aggregator.add_unmerged_pointcloud(pcl)\n",
    "            all_pointclouds.append(pcl)\n",
    "                        \n",
    "## TESTING 11/22: merge all instances\n",
    "# aggregator.gather_pointclouds()\n",
    "# aggregator.aggregate_all()\n",
    "# projector.visualize(aggregator.main)\n",
    "\n",
    "\n",
    "count = 0\n",
    "all_pointclouds = all_pointclouds[:-3]\n",
    "\n",
    "while len(all_pointclouds) > 1:\n",
    "    print(len(all_pointclouds))\n",
    "    new_pointclouds = []\n",
    "    eps = 0.0001 * (10 ** count)\n",
    "    for i in range(1, len(all_pointclouds), 2):\n",
    "        print(f\"i = {i}\")\n",
    "        if i >= len(all_pointclouds):\n",
    "            break\n",
    "        target = all_pointclouds[i] # 1\n",
    "        source = all_pointclouds[i - 1] # 0 \n",
    "\n",
    "        # transform = world_transforms[os.path.basename(color_images[i])]\n",
    "        # all_pointclouds[i].transform(transform)\n",
    "        \n",
    "        source_transform = world_transforms[os.path.basename(rgb_images[i - 1])] if count == 0 else np.eye(4)\n",
    " \n",
    "        target_transform = world_transforms[os.path.basename(rgb_images[i])] if count == 0 else np.eye(4)\n",
    "        \n",
    "        eps += i / 1000\n",
    "        \n",
    "        target.transform(target_transform)\n",
    "\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "            source._pcl, target._pcl, eps, source_transform, o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "        )\n",
    "\n",
    "        source.transform(reg_p2p.transformation)\n",
    "        new_pointclouds.append(target + source)\n",
    "\n",
    "    all_pointclouds = new_pointclouds\n",
    "    count += 1\n",
    "\n",
    "    projector.visualize(all_pointclouds)\n",
    "print(len(all_pointclouds))\n",
    "projector.visualize(all_pointclouds)\n",
    "\n",
    "# allpoints = []\n",
    "\n",
    "# for label in pointclouds:\n",
    "#     for pcl in pointclouds[label]:\n",
    "#         allpoints.append(pcl)\n",
    "\n",
    "# projector.visualize(allpoints)\n",
    "\n",
    "## Show pointclouds\n",
    "# projector.visualize(pointclouds['chair'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rgb_images)\n",
    "# print('objects', aggregator._scene.keys())\n",
    "# projector.visualize(aggregator._scene['couch'])\n",
    "\n",
    "# res = PointCloud()\n",
    "# res.label = \"chair\"\n",
    "# for pcl in allpoints:\n",
    "#     res += pcl\n",
    "# rgb_images.sort(key=lambda x: int(os.path.basename(x)[:-4]))\n",
    "# print(rgb_images)\n",
    "\n",
    "# projector.visualize(aggregator._unmerged_pointclouds['couch'])\n",
    "# aggregator.gather_pointclouds()\n",
    "# aggregator.aggregate_all()\n",
    "# projector.visualize(aggregator._scene['couch'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
